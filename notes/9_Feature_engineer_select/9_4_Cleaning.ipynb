{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Principles of Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.4  Imputing Missing Values in a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world data can contain *missing values* due to human error in collection, storage issues, faulty sensors, etc.,  If a supervised learning datapoint is missing its *output* value - e.g., if a classification datapoint is missing its *label* - there can be little we can do to salvage the datapoint, and usually such a corrupted datapoint is thrown away.  Likewise, if a large number of the values of an input are missing the data is best discarded.  However a datapoint whose input is missing just a handful of *input (feature) values* can be salvaged, and such points should be salvaged since data is often a scarce resource.  Because machine learning models take in only numerical valued objects, in order for us to be able to use a datapoint with missing entries in training any machine learning model we must *fill in missing input features with numerical values* (this is often called *imputing*).  But what values should we set missing entries too? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code cell will not be shown in the HTML version of this notebook\n",
    "# append path for local library, data, and image import\n",
    "import sys\n",
    "sys.path.append('./chapter_9_library') \n",
    "sys.path.append('./chapter_9_images') \n",
    "sys.path.append('./chapter_9_datasets') \n",
    "\n",
    "# image paths\n",
    "image_path_1 = \"chapter_9_images/mean_inpute_stadnard_normal.png\"\n",
    "\n",
    "# standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import IPython, copy\n",
    "\n",
    "# standard imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# import autograd-wrapped numpy\n",
    "import autograd.numpy as np\n",
    "\n",
    "# this is needed to compensate for matplotlib notebook's tendancy to blow up images when plotted inline\n",
    "%matplotlib inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing using the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Suppose we have a set of $P$ inputs, each of which is $N$ dimensional, and say that the $j^{th}$ point has is missing its $n^{th}$ input feature value.  In other words, the value $x_{j,n}$ is missing from our input data.  A reasonable value to fill for this missing entry is simply the *average of the dataset along this dimension*, since this can roughly be considered the 'standard' value of our input along this dimension.  If in particular we use the *mean* $\\mu_n$ of our input along this dimension (ignoring our missing entry $x_{j,n}$) we set $x_{j,n} = \\mu_n$ where\n",
    " \n",
    "\\begin{equation}\n",
    "\\mu_n = \\frac{1}{P-1} \\sum_{\\underset{p \\neq j}{j=1}}^P x_{p,n}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is often called *mean-imputation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how, in mean-imputing $x_{p,n}$, that the mean value of the entire $n^{th}$ feature of the input with the inputed value *does not change*.  We can see this by simply computing the mean of the new $n^{th}$ input dimension as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{P} \\sum_{p=1}^P x_{p,n} = \\frac{1}{P} \\sum_{\\underset{p \\neq j}{j=1}}^P  x_{p,n} + \\frac{1}{P} x_{j,n} = \\frac{P - 1}{P} \\frac{1}{P-1} \\sum_{\\underset{p \\neq j}{j=1}}^Px_{p,n} + \\frac{1}{P} x_{j,n} = \\frac{P - 1}{P} \\mu_n + \\frac{1}{P} \\mu_n = \\mu_n.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that if we impute missing values of a dataset *using the mean along each input dimension* that if we then *standard normalize* this dataset, *all values imputed with the mean become exactly zero*.  Because the mean of an input feature that has been imputed with mean values does not change (as shown above), when we *mean-center* the dataset (the first step in standard normalization) by *subtracting* the mean of this input dimension the resulting value becomes exactly zero (this is illustrated in the figure below for a simple case where $N = 2$).  Any model weight touching such a mean-imputed entry is then comopletely nullified numerically.  In other words, in training any mean-imputed value will not directly contribute to the tuning of its associated feature weight (provided the input has been standard normalized).  This is very desireable given that such values were missing to begin with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally if our input is missing multiple entries along this dimension we can just as reasonably replace each one with the mean along the $n^{th}$ feature, where in computing this mean we once again exclude missing values.  Again one can see that with these imputed values the *mean along this dimension does not change*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"chapter_9_images/mean_inpute_stadnard_normal.png\" width=\"100%\" height=\"auto\" alt=\"\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.HTML('''<img src=\"''' + image_path_1 + '''\" width=\"100%\" height=\"auto\" alt=\"\"/>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <figcaption>   \n",
    "<strong>Figure 1:</strong> <em> \n",
    "(left panel) The input of a prototypical $N = 2$ dimensional dataset where a single point $\\mathbf{x}_j$,  drawn as a *hollow* red dot, is missing its second entry.  The mean-imputed version of this point is then shown as a *filled-in* red point.  (right panel) By standard-normalizing such a dataset the mean-imputed value of $\\mathbf{x}_j$ becomes *exactly equal to zero*.\n",
    "</em>  </figcaption> \n",
    "</figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "68px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
